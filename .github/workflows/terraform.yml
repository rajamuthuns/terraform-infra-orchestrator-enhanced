name: Terraform Deploy

on:
  push:
    branches:
      - dev
      - staging
      - production
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target environment test'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - staging
          - production
      action:
        description: 'Pipeline action'
        required: true
        default: 'plan-and-apply'
        type: choice
        options:
          - setup-only
          - plan-only
          - plan-and-apply
          - destroy
      skip_setup:
        description: 'Skip setup stage (if backends already exist)'
        required: false
        default: false
        type: boolean
      create_promotion_pr:
        description: 'Create PR to next environment after successful deployment'
        required: false
        default: true
        type: boolean

env:
  TF_VERSION: "1.6.0"
  AWS_DEFAULT_REGION: "us-east-1"

jobs:
  setup-backends:
    name: Setup Common Terraform Backend
    runs-on: ubuntu-latest
    if: |
      (github.event_name == 'workflow_dispatch' && github.event.inputs.skip_setup != 'true') ||
      (github.event_name == 'push')
    outputs:
      setup-success: ${{ steps.setup-result.outputs.success }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PRIVATE_REPO_TOKEN }}

      - name: Configure Git Authentication
        run: |
          git config --global url."https://${{ secrets.PRIVATE_REPO_TOKEN }}@github.com/".insteadOf "https://github.com/"
          git config --global credential.helper store
          git config --global user.email "github-actions@github.com"
          git config --global user.name "GitHub Actions"
          
          mkdir -p ~/.git
          echo "https://${{ secrets.PRIVATE_REPO_TOKEN }}@github.com" > ~/.git-credentials
          chmod 600 ~/.git-credentials
          
          echo "GIT_CONFIG_GLOBAL=$HOME/.gitconfig" >> $GITHUB_ENV
          echo "GIT_ASKPASS=echo" >> $GITHUB_ENV
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}

      - name: Determine target environment
        id: target-env
        run: |
          echo "ðŸ”§ Setup Backend Job Started"
          echo "ðŸ“‹ Event: ${{ github.event_name }}"
          echo "ðŸ“‹ Action: ${{ github.event.inputs.action }}"
          echo "ðŸ“‹ Skip Setup: ${{ github.event.inputs.skip_setup }}"
          
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            ENV="${{ github.event.inputs.environment }}"
          else
            case "${{ github.ref_name }}" in
              "dev") ENV="dev" ;;
              "staging") ENV="staging" ;;
              "production") ENV="production" ;;
              *) ENV="dev" ;;
            esac
          fi
          echo "environment=$ENV" >> $GITHUB_OUTPUT
          echo "âœ… Target environment: $ENV"
      - name: Setup common backend configuration
        run: |
          echo "ðŸ”§ Setting up common backend resources"
          echo "ðŸ“‹ Event: ${{ github.event_name }}"
          echo "ðŸ“‹ Action: ${{ github.event.inputs.action }}"
          
          # Ensure config file exists for shared services account
          if [ ! -f "config/aws-accounts.json" ]; then
            echo "âŒ config/aws-accounts.json not found"
            echo "This file is required to determine the shared services account for backend resources"
            echo "Please create config/aws-accounts.json with shared_services account configuration"
            exit 1
          fi
          
          # Get shared services account ID from config (controlled by platform team)
          SHARED_SERVICES_ACCOUNT_ID=$(jq -r '.shared_services.account_id' config/aws-accounts.json)
          
          if [ "$SHARED_SERVICES_ACCOUNT_ID" = "null" ] || [ "$SHARED_SERVICES_ACCOUNT_ID" = "REPLACE_WITH_SHARED_SERVICES_ACCOUNT_ID" ]; then
            echo "âŒ Shared services account ID not configured in config/aws-accounts.json"
            echo "Please update the shared_services.account_id field with the actual shared services account ID"
            echo "This ensures backend resources are created in the correct account controlled by the platform team"
            exit 1
          fi
          
          echo "ðŸ“‹ Shared Services Account (for backend): $SHARED_SERVICES_ACCOUNT_ID"
          echo "ðŸ›ï¸ This account is controlled by the platform/orchestrator team"
          
          # Use single common resources in shared services account
          COMMON_BUCKET_NAME="terraform-state-central-multi-env"
          COMMON_DYNAMODB_TABLE="terraform-state-locks-common"
          
          echo "ðŸ“¦ Common bucket: $COMMON_BUCKET_NAME"
          echo "ðŸ—„ï¸ Common DynamoDB table: $COMMON_DYNAMODB_TABLE"
          
          # Assume role in shared services account to create backend resources
          SHARED_SERVICES_ROLE_ARN="arn:aws:iam::${SHARED_SERVICES_ACCOUNT_ID}:role/OrganizationAccountAccessRole"
          
          echo "ðŸ” Assuming role in shared services account: $SHARED_SERVICES_ROLE_ARN"
          if SHARED_CREDENTIALS=$(aws sts assume-role \
            --role-arn "$SHARED_SERVICES_ROLE_ARN" \
            --role-session-name "backend-setup-shared-services" \
            --query 'Credentials.[AccessKeyId,SecretAccessKey,SessionToken]' \
            --output text 2>&1); then
            
            export AWS_ACCESS_KEY_ID=$(echo $SHARED_CREDENTIALS | cut -d' ' -f1)
            export AWS_SECRET_ACCESS_KEY=$(echo $SHARED_CREDENTIALS | cut -d' ' -f2)
            export AWS_SESSION_TOKEN=$(echo $SHARED_CREDENTIALS | cut -d' ' -f3)
            echo "âœ… Successfully assumed role in shared services account"
          else
            echo "âŒ Failed to assume role in shared services account"
            echo "Error: $SHARED_CREDENTIALS"
            exit 1
          fi
          
          # Create common S3 bucket if it doesn't exist
          echo "Checking S3 bucket: $COMMON_BUCKET_NAME"
          if aws s3api head-bucket --bucket "$COMMON_BUCKET_NAME" 2>/dev/null; then
            echo "âœ… S3 bucket $COMMON_BUCKET_NAME already exists"
          else
            echo "Creating S3 bucket: $COMMON_BUCKET_NAME"
            aws s3api create-bucket --bucket "$COMMON_BUCKET_NAME" --region us-east-1
            aws s3api put-bucket-versioning --bucket "$COMMON_BUCKET_NAME" --versioning-configuration Status=Enabled
            aws s3api put-bucket-encryption --bucket "$COMMON_BUCKET_NAME" --server-side-encryption-configuration '{
              "Rules": [{
                "ApplyServerSideEncryptionByDefault": {
                  "SSEAlgorithm": "AES256"
                }
              }]
            }'
            aws s3api put-public-access-block --bucket "$COMMON_BUCKET_NAME" --public-access-block-configuration \
              "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"
            echo "âœ… S3 bucket created and configured"
          fi
          
          # Create common DynamoDB table in shared services account
          echo "Checking DynamoDB table: $COMMON_DYNAMODB_TABLE in shared services account"
          if aws dynamodb describe-table --table-name "$COMMON_DYNAMODB_TABLE" 2>/dev/null; then
            echo "âœ… DynamoDB table $COMMON_DYNAMODB_TABLE already exists"
          else
            echo "Creating DynamoDB table: $COMMON_DYNAMODB_TABLE in shared services account"
            aws dynamodb create-table \
              --table-name "$COMMON_DYNAMODB_TABLE" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST \
              --sse-specification Enabled=true
            
            # Wait for table to be active
            echo "Waiting for DynamoDB table to become active..."
            aws dynamodb wait table-exists --table-name "$COMMON_DYNAMODB_TABLE"
            echo "âœ… DynamoDB table created and active in shared services account"
          fi
          
          # Update bucket policy for cross-account access from environment accounts
          echo "Setting up cross-account bucket policy..."
          
          # Build list of account ARNs that need access to the backend
          ACCOUNT_ARNS="\"arn:aws:iam::${SHARED_SERVICES_ACCOUNT_ID}:root\""
          
          # Add current user/account for administrative access
          CURRENT_USER_ARN=$(aws sts get-caller-identity --query Arn --output text)
          CURRENT_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ACCOUNT_ARNS="${ACCOUNT_ARNS},\"${CURRENT_USER_ARN}\""
          ACCOUNT_ARNS="${ACCOUNT_ARNS},\"arn:aws:iam::${CURRENT_ACCOUNT_ID}:root\""
          echo "  - Added access for current user: $CURRENT_USER_ARN"
          echo "  - Added access for current account: $CURRENT_ACCOUNT_ID"
          
          # Add master account (345918514280) for terraform init access
          MASTER_ACCOUNT_ID="345918514280"
          ACCOUNT_ARNS="${ACCOUNT_ARNS},\"arn:aws:iam::${MASTER_ACCOUNT_ID}:root\""
          echo "  - Added access for master account: $MASTER_ACCOUNT_ID"
          
          # Add all accounts from config (including org_master for pipeline access)
          for env in $(jq -r 'keys[]' config/aws-accounts.json); do
            if [ "$env" != "shared_services" ]; then
              account_id=$(jq -r ".${env}.account_id" config/aws-accounts.json)
              if [ "$account_id" != "null" ] && [ "$account_id" != "REPLACE_WITH_PRODUCTION_ACCOUNT_ID" ]; then
                ACCOUNT_ARNS="${ACCOUNT_ARNS},\"arn:aws:iam::${account_id}:role/OrganizationAccountAccessRole\""
                # Also add root access for the account (for broader access patterns)
                ACCOUNT_ARNS="${ACCOUNT_ARNS},\"arn:aws:iam::${account_id}:root\""
                echo "  - Added access for $env account: $account_id (via OrganizationAccountAccessRole and root)"
              fi
            fi
          done
          
          # Apply bucket policy for cross-account and cross-region access
          aws s3api put-bucket-policy --bucket "$COMMON_BUCKET_NAME" --policy "{
            \"Version\": \"2012-10-17\",
            \"Statement\": [
              {
                \"Sid\": \"AllowCrossAccountAccess\",
                \"Effect\": \"Allow\",
                \"Principal\": {
                  \"AWS\": [${ACCOUNT_ARNS}]
                },
                \"Action\": [
                  \"s3:GetObject\",
                  \"s3:PutObject\",
                  \"s3:DeleteObject\",
                  \"s3:ListBucket\",
                  \"s3:GetBucketLocation\",
                  \"s3:ListBucketVersions\",
                  \"s3:GetBucketVersioning\",
                  \"s3:PutBucketVersioning\"
                ],
                \"Resource\": [
                  \"arn:aws:s3:::${COMMON_BUCKET_NAME}\",
                  \"arn:aws:s3:::${COMMON_BUCKET_NAME}/*\"
                ]
              }
            ]
          }"
          
          echo "âœ… Cross-account bucket policy configured"
          
          # Apply DynamoDB resource policy for cross-account access
          echo "Setting up DynamoDB resource policy for cross-account access..."
          
          # Build list of account ARNs for DynamoDB access (reuse the same list)
          DYNAMODB_ACCOUNT_ARNS="${ACCOUNT_ARNS}"
          
          # Apply DynamoDB resource policy
          aws dynamodb put-resource-policy \
            --resource-arn "arn:aws:dynamodb:us-east-1:${SHARED_SERVICES_ACCOUNT_ID}:table/${COMMON_DYNAMODB_TABLE}" \
            --policy "{
              \"Version\": \"2012-10-17\",
              \"Statement\": [
                {
                  \"Sid\": \"AllowCrossAccountDynamoDBAccess\",
                  \"Effect\": \"Allow\",
                  \"Principal\": {
                    \"AWS\": [${DYNAMODB_ACCOUNT_ARNS}]
                  },
                  \"Action\": [
                    \"dynamodb:GetItem\",
                    \"dynamodb:PutItem\",
                    \"dynamodb:DeleteItem\",
                    \"dynamodb:DescribeTable\"
                  ],
                  \"Resource\": \"arn:aws:dynamodb:us-east-1:${SHARED_SERVICES_ACCOUNT_ID}:table/${COMMON_DYNAMODB_TABLE}\"
                }
              ]
            }" 2>/dev/null && echo "âœ… DynamoDB resource policy configured" || {
            echo "âš ï¸ DynamoDB resource policy not supported in this region or account"
            echo "ðŸ’¡ Alternative: Ensure OrganizationAccountAccessRole in environment accounts has DynamoDB permissions"
          }
          
          # Create common backend configuration file (same for all environments)
          cat > "shared/backend-common.hcl" << EOF
          # Common backend configuration for all environments
          # Workspaces handle environment separation
          bucket         = "${COMMON_BUCKET_NAME}"
          key            = "terraform.tfstate"
          region         = "us-east-1"
          dynamodb_table = "${COMMON_DYNAMODB_TABLE}"
          encrypt        = true
          
          # Workspace configuration - creates separate state files per workspace
          workspace_key_prefix = "environments"
          
          
          skip_credentials_validation = false
          skip_metadata_api_check = false
          skip_region_validation = false
          use_path_style = false
          max_retries = 5
          EOF
          
          echo "âœ… Common backend setup completed"
          echo "ðŸ“¦ Bucket: $COMMON_BUCKET_NAME (in shared services account: $SHARED_SERVICES_ACCOUNT_ID)"
          echo "ðŸ—„ï¸ DynamoDB: $COMMON_DYNAMODB_TABLE (in shared services account: $SHARED_SERVICES_ACCOUNT_ID)"
          echo "ðŸ¢ Workspaces will handle environment separation"
          echo "ðŸ›ï¸ Backend resources controlled by platform team via config/aws-accounts.json"
          echo "ðŸ“„ Common backend config created: shared/backend-common.hcl"
      - name: Upload backend artifacts
        uses: actions/upload-artifact@v4
        with:
          name: terraform-backends
          path: shared/backend-common.hcl
          retention-days: 7

      - name: Set setup success output
        id: setup-result
        run: echo "success=true" >> $GITHUB_OUTPUT

  determine-environment:
    name: Determine Environment
    runs-on: ubuntu-latest
    outputs:
      environment: ${{ steps.env.outputs.environment }}
      tfvars-file: ${{ steps.env.outputs.tfvars-file }}
      needs-approval: ${{ steps.env.outputs.needs-approval }}
    steps:
      - name: Determine environment from branch or input
        id: env
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            ENV="${{ github.event.inputs.environment }}"
          else
            case "${{ github.ref_name }}" in
              "dev") ENV="dev" ;;
              "staging") ENV="staging" ;;
              "production") ENV="production" ;;
              *) ENV="dev" ;;
            esac
          fi
          
          echo "environment=$ENV" >> $GITHUB_OUTPUT
          
          case "$ENV" in
            "dev")
              echo "tfvars-file=dev-terraform.tfvars" >> $GITHUB_OUTPUT
              echo "needs-approval=false" >> $GITHUB_OUTPUT
              ;;
            "staging")
              echo "tfvars-file=stg-terraform.tfvars" >> $GITHUB_OUTPUT
              echo "needs-approval=true" >> $GITHUB_OUTPUT
              ;;
            "production")
              echo "tfvars-file=prod-terraform.tfvars" >> $GITHUB_OUTPUT
              echo "needs-approval=true" >> $GITHUB_OUTPUT
              ;;
          esac
          
          echo "Determined environment: $ENV"
  terraform-plan:
    name: Terraform Plan - ${{ needs.determine-environment.outputs.environment }}
    runs-on: ubuntu-latest
    needs: [setup-backends, determine-environment]
    if: |
      always() && 
      (
        (github.event_name == 'workflow_dispatch' && (github.event.inputs.action == 'plan-and-apply' || github.event.inputs.action == 'plan-only')) ||
        (github.event_name == 'push')
      ) &&
      (
        (needs.setup-backends.result == 'success') ||
        (needs.setup-backends.result == 'skipped')
      )
    outputs:
      plan-success: ${{ steps.plan-result.outputs.success }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PRIVATE_REPO_TOKEN }}

      - name: Configure Git Authentication
        run: |
          git config --global url."https://${{ secrets.PRIVATE_REPO_TOKEN }}@github.com/".insteadOf "https://github.com/"
          git config --global credential.helper store
          git config --global user.email "github-actions@github.com"
          git config --global user.name "GitHub Actions"
          
          mkdir -p ~/.git
          echo "https://${{ secrets.PRIVATE_REPO_TOKEN }}@github.com" > ~/.git-credentials
          chmod 600 ~/.git-credentials
          
          echo "GIT_CONFIG_GLOBAL=$HOME/.gitconfig" >> $GITHUB_ENV
          echo "GIT_ASKPASS=echo" >> $GITHUB_ENV
      - name: Download backend artifacts
        if: needs.setup-backends.result == 'success'
        uses: actions/download-artifact@v4
        with:
          name: terraform-backends
          path: shared/
        continue-on-error: true

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}



      - name: Check backend configuration
        run: |
          BACKEND_FILE="shared/backend-common.hcl"
          if [ ! -f "$BACKEND_FILE" ]; then
            echo "âŒ Common backend configuration file not found: $BACKEND_FILE"
            echo "This usually means the setup stage was skipped or failed."
            echo "Available backend files:"
            ls -la shared/backend-*.hcl 2>/dev/null || echo "No backend files found"
            exit 1
          fi
          
          echo "âœ… Common backend configuration looks good:"
          cat "$BACKEND_FILE"
          echo "ðŸ¢ Environment isolation will be handled by workspace: ${{ needs.determine-environment.outputs.environment }}"
      - name: Terraform Init and Plan
        id: plan
        run: |
          if [ ! -f tfvars/${{ needs.determine-environment.outputs.tfvars-file }} ]; then
            echo "Error: tfvars file not found: tfvars/${{ needs.determine-environment.outputs.tfvars-file }}"
            exit 1
          fi
          
          # Get target account from tfvars
          TARGET_ACCOUNT_ID=$(grep -E '^account_id\s*=' "tfvars/${{ needs.determine-environment.outputs.tfvars-file }}" | sed 's/.*=\s*"\([^"]*\)".*/\1/' | tr -d ' ')
          CURRENT_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          
          # Get shared services account ID from config
          SHARED_SERVICES_ACCOUNT_ID=$(jq -r '.shared_services.account_id' config/aws-accounts.json)
          
          echo "ðŸ”§ Using two-stage credential approach:"
          echo "ðŸ“‹ Org Master Account: $CURRENT_ACCOUNT_ID (for provider operations)"
          echo "ðŸ›ï¸ Shared Services Account: $SHARED_SERVICES_ACCOUNT_ID (for backend access only)"
          echo "ðŸŽ¯ Target Deployment Account: $TARGET_ACCOUNT_ID"
          
          # Create backend configuration with assume_role for shared services account
          # This allows Terraform to assume the role for backend operations while keeping
          # org master credentials for provider operations
          SHARED_SERVICES_ROLE_ARN="arn:aws:iam::${SHARED_SERVICES_ACCOUNT_ID}:role/OrganizationAccountAccessRole"
          
          echo "ðŸ”§ Creating backend configuration with assume_role for shared services account..."
          cat > "backend-with-assume-role.hcl" << EOF
          bucket         = "terraform-state-central-multi-env"
          key            = "terraform.tfstate"
          region         = "us-east-1"
          dynamodb_table = "terraform-state-locks-common"
          encrypt        = true
          workspace_key_prefix = "environments"
          
          assume_role = {
            role_arn = "${SHARED_SERVICES_ROLE_ARN}"
            session_name = "terraform-plan-backend-access"
          }
          
          skip_credentials_validation = false
          skip_metadata_api_check = false
          skip_region_validation = false
          use_path_style = false
          max_retries = 5
          EOF
          
          echo "âœ… Backend configuration created with assume_role for shared services account"
          echo "ðŸ”§ This allows:"
          echo "  - Backend operations: Assume shared services account role for S3/DynamoDB access"
          echo "  - Provider operations: Use org master credentials to assume target account role"
          
          # Initialize with backend assume_role configuration
          echo "ðŸ”§ Initializing with backend assume_role configuration..."
          terraform init -backend-config=backend-with-assume-role.hcl
          
          # Set up workspace
          if terraform workspace list | grep -q "^\s*${{ needs.determine-environment.outputs.environment }}\s*$"; then
            terraform workspace select ${{ needs.determine-environment.outputs.environment }}
          else
            terraform workspace new ${{ needs.determine-environment.outputs.environment }}
          fi
          
          # Run terraform plan with org master credentials
          # Backend operations will use assume_role to shared services account
          # Provider operations will use org master credentials to assume target account role
          echo "ðŸ”§ Running terraform plan with org master credentials..."
          echo "  - Backend: Will assume shared services account role for state/locking"
          echo "  - Provider: Will assume target deployment account role for resources"
          terraform plan -var-file=tfvars/${{ needs.determine-environment.outputs.tfvars-file }} -out=tfplan
          
          terraform show -no-color tfplan > plan-output.txt
          echo "## Terraform Plan Summary" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ needs.determine-environment.outputs.environment }}" >> $GITHUB_STEP_SUMMARY
          echo "**Tfvars File:** ${{ needs.determine-environment.outputs.tfvars-file }}" >> $GITHUB_STEP_SUMMARY
      - name: Upload plan artifacts
        uses: actions/upload-artifact@v4
        with:
          name: tfplan-${{ needs.determine-environment.outputs.environment }}
          path: |
            tfplan
            plan-output.txt
          retention-days: 7

      - name: Set plan success output
        id: plan-result
        run: echo "success=true" >> $GITHUB_OUTPUT

  deploy:
    name: Deploy - ${{ needs.determine-environment.outputs.environment }}
    runs-on: ubuntu-latest
    needs: [setup-backends, determine-environment, terraform-plan]
    if: |
      always() && 
      needs.terraform-plan.outputs.plan-success == 'true' &&
      (
        (github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'plan-and-apply') ||
        (github.event_name == 'push')
      ) &&
      (github.event_name != 'workflow_dispatch' || github.event.inputs.action != 'plan-only')
    environment: 
      name: ${{ needs.determine-environment.outputs.environment }}
    outputs:
      deploy-success: ${{ steps.apply-result.outputs.success }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PRIVATE_REPO_TOKEN }}

      - name: Configure Git Authentication
        run: |
          git config --global url."https://${{ secrets.PRIVATE_REPO_TOKEN }}@github.com/".insteadOf "https://github.com/"
          git config --global credential.helper store
          git config --global user.email "github-actions@github.com"
          git config --global user.name "GitHub Actions"
          
          mkdir -p ~/.git
          echo "https://${{ secrets.PRIVATE_REPO_TOKEN }}@github.com" > ~/.git-credentials
          chmod 600 ~/.git-credentials
          
          echo "GIT_CONFIG_GLOBAL=$HOME/.gitconfig" >> $GITHUB_ENV
          echo "GIT_ASKPASS=echo" >> $GITHUB_ENV
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Download backend artifacts
        if: needs.setup-backends.result == 'success'
        uses: actions/download-artifact@v4
        with:
          name: terraform-backends
          path: shared/
        continue-on-error: true

      - name: Download plan artifacts
        uses: actions/download-artifact@v4
        with:
          name: tfplan-${{ needs.determine-environment.outputs.environment }}
          path: ./

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}


      - name: Check backend configuration
        run: |
          BACKEND_FILE="shared/backend-common.hcl"
          if [ ! -f "$BACKEND_FILE" ]; then
            echo "âŒ Common backend configuration file not found: $BACKEND_FILE"
            echo "Available backend files:"
            ls -la shared/backend-*.hcl 2>/dev/null || echo "No backend files found"
            exit 1
          fi
          
          echo "âœ… Common backend configuration looks good:"
          cat "$BACKEND_FILE"
          echo "ðŸ¢ Environment isolation will be handled by workspace: ${{ needs.determine-environment.outputs.environment }}"
      - name: Terraform Init
        run: |
          # Get target account from tfvars
          TARGET_ACCOUNT_ID=$(grep -E '^account_id\s*=' "tfvars/${{ needs.determine-environment.outputs.tfvars-file }}" | sed 's/.*=\s*"\([^"]*\)".*/\1/' | tr -d ' ')
          CURRENT_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          
          # Get shared services account ID from config
          SHARED_SERVICES_ACCOUNT_ID=$(jq -r '.shared_services.account_id' config/aws-accounts.json)
          
          echo "ðŸ”§ Using backend assume_role approach:"
          echo "ðŸ“‹ Org Master Account: $CURRENT_ACCOUNT_ID (for provider operations)"
          echo "ðŸ›ï¸ Shared Services Account: $SHARED_SERVICES_ACCOUNT_ID (for backend access via assume_role)"
          echo "ðŸŽ¯ Target Deployment Account: $TARGET_ACCOUNT_ID"
          
          # Create backend configuration with assume_role for shared services account
          SHARED_SERVICES_ROLE_ARN="arn:aws:iam::${SHARED_SERVICES_ACCOUNT_ID}:role/OrganizationAccountAccessRole"
          
          echo "ðŸ”§ Creating backend configuration with assume_role for shared services account..."
          cat > "backend-with-assume-role.hcl" << EOF
          bucket         = "terraform-state-central-multi-env"
          key            = "terraform.tfstate"
          region         = "us-east-1"
          dynamodb_table = "terraform-state-locks-common"
          encrypt        = true
          workspace_key_prefix = "environments"
          
          assume_role = {
            role_arn = "${SHARED_SERVICES_ROLE_ARN}"
            session_name = "terraform-deploy-backend-access"
          }
          
          skip_credentials_validation = false
          skip_metadata_api_check = false
          skip_region_validation = false
          use_path_style = false
          max_retries = 5
          EOF
          
          echo "âœ… Backend configuration created with assume_role for shared services account"
          
          # Initialize with backend assume_role configuration
          echo "ðŸ”§ Initializing with backend assume_role configuration..."
          terraform init -backend-config=backend-with-assume-role.hcl
        env:
          GITHUB_TOKEN: ${{ secrets.PRIVATE_REPO_TOKEN }}
          GIT_TOKEN: ${{ secrets.PRIVATE_REPO_TOKEN }}

      - name: Terraform Workspace
        run: |
          echo "Setting up workspace: ${{ needs.determine-environment.outputs.environment }}"
          if terraform workspace list | grep -q "^\s*${{ needs.determine-environment.outputs.environment }}\s*$"; then
            echo "Workspace '${{ needs.determine-environment.outputs.environment }}' already exists, selecting it"
            terraform workspace select ${{ needs.determine-environment.outputs.environment }}
          else
            echo "Creating new workspace: ${{ needs.determine-environment.outputs.environment }}"
            terraform workspace new ${{ needs.determine-environment.outputs.environment }}
          fi

      - name: Terraform Apply
        id: apply
        run: |
          echo "ðŸ”§ Running terraform apply with org master credentials..."
          echo "  - Backend: Will assume shared services account role for state/locking"
          echo "  - Provider: Will assume target deployment account role for resources"
          if [ -f tfplan ]; then
            echo "Using existing plan file..."
            terraform apply -auto-approve tfplan
          else
            echo "No plan file found, running apply with tfvars..."
            terraform apply -auto-approve -var-file=tfvars/${{ needs.determine-environment.outputs.tfvars-file }}
          fi
      - name: Get outputs
        id: outputs
        run: |
          terraform output -json > outputs.json
          
          ALB_ENDPOINTS=$(terraform output -json alb_endpoints 2>/dev/null || echo '{}')
          if [ "$ALB_ENDPOINTS" != "{}" ]; then
            FIRST_ALB_URL=$(echo $ALB_ENDPOINTS | jq -r 'to_entries | .[0].value // empty')
            if [ -n "$FIRST_ALB_URL" ]; then
              echo "alb_url=http://$FIRST_ALB_URL" >> $GITHUB_OUTPUT
            fi
          fi
      - name: Upload outputs
        uses: actions/upload-artifact@v4
        with:
          name: terraform-outputs-${{ needs.determine-environment.outputs.environment }}
          path: outputs.json
          retention-days: 30

      - name: Set deploy success output
        id: apply-result
        run: echo "success=true" >> $GITHUB_OUTPUT

  destroy-approval:
    name: Destroy Approval Required
    runs-on: ubuntu-latest
    needs: [setup-backends, determine-environment]
    if: |
      always() && 
      github.event_name == 'workflow_dispatch' && 
      github.event.inputs.action == 'destroy' &&
      (needs.setup-backends.result == 'success' || needs.setup-backends.result == 'skipped')
    environment: 
      name: destroy-${{ needs.determine-environment.outputs.environment }}
    outputs:
      approved: ${{ steps.approval.outputs.approved }}
    steps:
      - name: Destroy Approval Required
        id: approval
        run: |
          echo "## âš ï¸ DESTROY CONFIRMATION REQUIRED" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ needs.determine-environment.outputs.environment }}" >> $GITHUB_STEP_SUMMARY
          echo "**Action:** DESTROY ALL INFRASTRUCTURE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### âš ï¸ WARNING" >> $GITHUB_STEP_SUMMARY
          echo "This will **PERMANENTLY DELETE** all infrastructure in the ${{ needs.determine-environment.outputs.environment }} environment!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ” What will be destroyed:" >> $GITHUB_STEP_SUMMARY
          echo "- All EC2 instances" >> $GITHUB_STEP_SUMMARY
          echo "- All Load Balancers" >> $GITHUB_STEP_SUMMARY
          echo "- All associated resources" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### âœ… To proceed with destroy:" >> $GITHUB_STEP_SUMMARY
          echo "1. This step requires manual approval in GitHub" >> $GITHUB_STEP_SUMMARY
          echo "2. Go to Repository Settings â†’ Environments" >> $GITHUB_STEP_SUMMARY
          echo "3. Create environment: destroy-${{ needs.determine-environment.outputs.environment }}" >> $GITHUB_STEP_SUMMARY
          echo "4. Add required reviewers (yourself)" >> $GITHUB_STEP_SUMMARY
          echo "5. Then approve this deployment to proceed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**This action cannot be undone!**" >> $GITHUB_STEP_SUMMARY
          echo "approved=true" >> $GITHUB_OUTPUT
  terraform-destroy:
    name: Terraform Destroy - ${{ needs.determine-environment.outputs.environment }}
    runs-on: ubuntu-latest
    needs: [setup-backends, determine-environment, destroy-approval]
    if: |
      always() && 
      needs.destroy-approval.outputs.approved == 'true' &&
      github.event_name == 'workflow_dispatch' && 
      github.event.inputs.action == 'destroy'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PRIVATE_REPO_TOKEN }}

      - name: Configure Git Authentication
        run: |
          git config --global url."https://${{ secrets.PRIVATE_REPO_TOKEN }}@github.com/".insteadOf "https://github.com/"
          git config --global credential.helper store
          git config --global user.email "github-actions@github.com"
          git config --global user.name "GitHub Actions"
          
          mkdir -p ~/.git
          echo "https://${{ secrets.PRIVATE_REPO_TOKEN }}@github.com" > ~/.git-credentials
          chmod 600 ~/.git-credentials
          
          echo "GIT_CONFIG_GLOBAL=$HOME/.gitconfig" >> $GITHUB_ENV
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Download backend artifacts
        if: needs.setup-backends.result == 'success'
        uses: actions/download-artifact@v4
        with:
          name: terraform-backends
          path: shared/
        continue-on-error: true

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}

      - name: Terraform Init
        run: |
          # Get target account from tfvars
          TARGET_ACCOUNT_ID=$(grep -E '^account_id\s*=' "tfvars/${{ needs.determine-environment.outputs.tfvars-file }}" | sed 's/.*=\s*"\([^"]*\)".*/\1/' | tr -d ' ')
          CURRENT_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          
          # Get shared services account ID from config
          SHARED_SERVICES_ACCOUNT_ID=$(jq -r '.shared_services.account_id' config/aws-accounts.json)
          
          echo "ðŸ”§ Using backend assume_role approach:"
          echo "ðŸ“‹ Org Master Account: $CURRENT_ACCOUNT_ID (for provider operations)"
          echo "ðŸ›ï¸ Shared Services Account: $SHARED_SERVICES_ACCOUNT_ID (for backend access via assume_role)"
          echo "ðŸŽ¯ Target Deployment Account: $TARGET_ACCOUNT_ID"
          
          # Create backend configuration with assume_role for shared services account
          # Use same filename as plan/deploy jobs to avoid "Backend configuration changed" error
          SHARED_SERVICES_ROLE_ARN="arn:aws:iam::${SHARED_SERVICES_ACCOUNT_ID}:role/OrganizationAccountAccessRole"
          
          echo "ðŸ”§ Creating backend configuration with assume_role for shared services account..."
          cat > "backend-with-assume-role.hcl" << EOF
          bucket         = "terraform-state-central-multi-env"
          key            = "terraform.tfstate"
          region         = "us-east-1"
          dynamodb_table = "terraform-state-locks-common"
          encrypt        = true
          workspace_key_prefix = "environments"
          
          assume_role = {
            role_arn = "${SHARED_SERVICES_ROLE_ARN}"
            session_name = "terraform-destroy-backend-access"
          }
          
          skip_credentials_validation = false
          skip_metadata_api_check = false
          skip_region_validation = false
          use_path_style = false
          max_retries = 5
          EOF
          
          echo "âœ… Backend configuration created with assume_role for shared services account"
          
          # Initialize with backend assume_role configuration
          echo "ðŸ”§ Initializing with backend assume_role configuration..."
          terraform init -backend-config=backend-with-assume-role.hcl
        env:
          GITHUB_TOKEN: ${{ secrets.PRIVATE_REPO_TOKEN }}
          GIT_TOKEN: ${{ secrets.PRIVATE_REPO_TOKEN }}

      - name: Terraform Workspace
        run: |
          echo "Setting up workspace: ${{ needs.determine-environment.outputs.environment }}"
          if terraform workspace list | grep -q "^\s*${{ needs.determine-environment.outputs.environment }}\s*$"; then
            echo "Workspace '${{ needs.determine-environment.outputs.environment }}' already exists, selecting it"
            terraform workspace select ${{ needs.determine-environment.outputs.environment }}
          else
            echo "Creating new workspace: ${{ needs.determine-environment.outputs.environment }}"
            terraform workspace new ${{ needs.determine-environment.outputs.environment }}
          fi
      - name: Confirm Destroy Action
        run: |
          echo "ðŸš¨ DESTROY ACTION CONFIRMED"
          echo "Environment: ${{ needs.determine-environment.outputs.environment }}"
          echo "Tfvars file: ${{ needs.determine-environment.outputs.tfvars-file }}"
          echo "This will destroy ALL resources in the ${{ needs.determine-environment.outputs.environment }} environment!"
      - name: Terraform Destroy
        run: |
          if [ ! -f tfvars/${{ needs.determine-environment.outputs.tfvars-file }} ]; then
            echo "Error: tfvars file not found: tfvars/${{ needs.determine-environment.outputs.tfvars-file }}"
            exit 1
          fi
          
          echo "ðŸ—‘ï¸ Starting terraform destroy..."
          echo "ðŸ“‹ Backend: Using shared services account via backend assume_role"
          echo "ðŸ“‹ Provider: Using org master credentials for cross-account assume role"
          echo "ðŸ“‹ Current provider account: $(aws sts get-caller-identity --query Account --output text)"
          
          # Use original tfvars (with dev account ID) so provider assumes role to dev account
          terraform destroy -auto-approve -var-file=tfvars/${{ needs.determine-environment.outputs.tfvars-file }}
          echo "âœ… Terraform destroy completed successfully"
  cleanup-and-retry-destroy:
    name: Cleanup ALB Logs and Retry Destroy
    runs-on: ubuntu-latest
    needs: [setup-backends, determine-environment, destroy-approval, terraform-destroy]
    if: |
      always() && 
      needs.destroy-approval.outputs.approved == 'true' &&
      needs.terraform-destroy.result == 'failure' &&
      github.event_name == 'workflow_dispatch' && 
      github.event.inputs.action == 'destroy'
    outputs:
      retry-success: ${{ steps.retry-result.outputs.success }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PRIVATE_REPO_TOKEN }}

      - name: Configure Git Authentication
        run: |
          git config --global url."https://${{ secrets.PRIVATE_REPO_TOKEN }}@github.com/".insteadOf "https://github.com/"
          git config --global credential.helper store
          git config --global user.email "github-actions@github.com"
          git config --global user.name "GitHub Actions"
          
          mkdir -p ~/.git
          echo "https://${{ secrets.PRIVATE_REPO_TOKEN }}@github.com" > ~/.git-credentials
          chmod 600 ~/.git-credentials
          
          echo "GIT_CONFIG_GLOBAL=$HOME/.gitconfig" >> $GITHUB_ENV
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Download backend artifacts
        if: needs.setup-backends.result == 'success'
        uses: actions/download-artifact@v4
        with:
          name: terraform-backends
          path: shared/
        continue-on-error: true

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}


      - name: Cleanup ALB access log buckets
        run: |
          echo "ðŸ§¹ Terraform destroy failed - cleaning up ALB access log buckets"
          echo "Environment: ${{ needs.determine-environment.outputs.environment }}"
          echo ""
          
          chmod +x scripts/cleanup-alb-logs.sh
          bash scripts/cleanup-alb-logs.sh "${{ needs.determine-environment.outputs.environment }}"
          
          # Additional cleanup for specific ALB log buckets
          echo "ðŸ§¹ Additional cleanup for ALB access log buckets..."
          
          # Find and clean all ALB access log buckets for this environment
          for bucket in $(aws s3api list-buckets --query "Buckets[?contains(Name, 'alb-access-logs') && contains(Name, '${{ needs.determine-environment.outputs.environment }}')].Name" --output text); do
            if [ -n "$bucket" ]; then
              echo "ðŸ—‘ï¸ Cleaning bucket: $bucket"
              
              # Delete all object versions
              aws s3api list-object-versions --bucket "$bucket" --query 'Versions[].{Key:Key,VersionId:VersionId}' --output text | while read key version; do
                if [ -n "$key" ] && [ -n "$version" ]; then
                  aws s3api delete-object --bucket "$bucket" --key "$key" --version-id "$version" || true
                fi
              done
              
              # Delete all delete markers
              aws s3api list-object-versions --bucket "$bucket" --query 'DeleteMarkers[].{Key:Key,VersionId:VersionId}' --output text | while read key version; do
                if [ -n "$key" ] && [ -n "$version" ]; then
                  aws s3api delete-object --bucket "$bucket" --key "$key" --version-id "$version" || true
                fi
              done
              
              echo "âœ… Cleaned bucket: $bucket"
            fi
          done
          
          echo "âœ… Additional ALB bucket cleanup completed"
      - name: Terraform Init
        run: |
          # Get target account from tfvars
          TARGET_ACCOUNT_ID=$(grep -E '^account_id\s*=' "tfvars/${{ needs.determine-environment.outputs.tfvars-file }}" | sed 's/.*=\s*"\([^"]*\)".*/\1/' | tr -d ' ')
          CURRENT_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          
          # Get shared services account ID from config
          SHARED_SERVICES_ACCOUNT_ID=$(jq -r '.shared_services.account_id' config/aws-accounts.json)
          
          echo "ðŸ”§ Using backend assume_role approach:"
          echo "ðŸ“‹ Org Master Account: $CURRENT_ACCOUNT_ID (for provider operations)"
          echo "ðŸ›ï¸ Shared Services Account: $SHARED_SERVICES_ACCOUNT_ID (for backend access via assume_role)"
          echo "ðŸŽ¯ Target Deployment Account: $TARGET_ACCOUNT_ID"
          
          # Create backend configuration with assume_role for shared services account
          # Use same filename as plan/deploy jobs to avoid "Backend configuration changed" error
          SHARED_SERVICES_ROLE_ARN="arn:aws:iam::${SHARED_SERVICES_ACCOUNT_ID}:role/OrganizationAccountAccessRole"
          
          echo "ðŸ”§ Creating backend configuration with assume_role for shared services account..."
          cat > "backend-with-assume-role.hcl" << EOF
          bucket         = "terraform-state-central-multi-env"
          key            = "terraform.tfstate"
          region         = "us-east-1"
          dynamodb_table = "terraform-state-locks-common"
          encrypt        = true
          workspace_key_prefix = "environments"
          
          assume_role = {
            role_arn = "${SHARED_SERVICES_ROLE_ARN}"
            session_name = "terraform-retry-destroy-backend-access"
          }
          
          skip_credentials_validation = false
          skip_metadata_api_check = false
          skip_region_validation = false
          use_path_style = false
          max_retries = 5
          EOF
          
          echo "âœ… Backend configuration created with assume_role for shared services account"
          
          # Initialize with backend assume_role configuration
          echo "ðŸ”§ Initializing with backend assume_role configuration..."
          terraform init -backend-config=backend-with-assume-role.hcl -reconfigure
        env:
          GITHUB_TOKEN: ${{ secrets.PRIVATE_REPO_TOKEN }}
          GIT_TOKEN: ${{ secrets.PRIVATE_REPO_TOKEN }}

      - name: Terraform Workspace
        run: |
          echo "Setting up workspace: ${{ needs.determine-environment.outputs.environment }}"
          if terraform workspace list | grep -q "^\s*${{ needs.determine-environment.outputs.environment }}\s*$"; then
            echo "Workspace '${{ needs.determine-environment.outputs.environment }}' already exists, selecting it"
            terraform workspace select ${{ needs.determine-environment.outputs.environment }}
          else
            echo "Creating new workspace: ${{ needs.determine-environment.outputs.environment }}"
            terraform workspace new ${{ needs.determine-environment.outputs.environment }}
          fi

      - name: Confirm Retry Destroy Action
        run: |
          echo "ðŸ”„ RETRY DESTROY ACTION CONFIRMED"
          echo "Environment: ${{ needs.determine-environment.outputs.environment }}"
          echo "Tfvars file: ${{ needs.determine-environment.outputs.tfvars-file }}"
          echo "This will retry destroying ALL resources in the ${{ needs.determine-environment.outputs.environment }} environment!"

      - name: Terraform Destroy
        run: |
          if [ ! -f tfvars/${{ needs.determine-environment.outputs.tfvars-file }} ]; then
            echo "Error: tfvars file not found: tfvars/${{ needs.determine-environment.outputs.tfvars-file }}"
            exit 1
          fi
          
          echo "ðŸ—‘ï¸ Retrying terraform destroy..."
          echo "ðŸ“‹ Backend: Using shared services account via backend assume_role"
          echo "ðŸ“‹ Provider: Using org master credentials for cross-account assume role"
          echo "ðŸ“‹ Current provider account: $(aws sts get-caller-identity --query Account --output text)"
          
          # Use original tfvars (with dev account ID) so provider assumes role to dev account
          terraform destroy -auto-approve -var-file=tfvars/${{ needs.determine-environment.outputs.tfvars-file }}
          echo "âœ… Terraform destroy retry completed successfully"

      - name: Set retry success output
        id: retry-result
        run: echo "success=true" >> $GITHUB_OUTPUT

      - name: Force cleanup remaining resources (if retry still fails)
        if: failure()
        run: |
          echo "âš ï¸ Terraform destroy retry failed. Attempting additional cleanup..."
          
          # Try destroy with -refresh=false to skip state refresh
          terraform destroy -auto-approve -var-file=tfvars/${{ needs.determine-environment.outputs.tfvars-file }} -refresh=false || true
          
          # Try targeted destroy of common problematic resources
          echo "ðŸŽ¯ Attempting targeted destroy of ALB resources..."
          terraform destroy -auto-approve -var-file=tfvars/${{ needs.determine-environment.outputs.tfvars-file }} -target=module.alb || true
          
          echo "ðŸŽ¯ Attempting targeted destroy of EC2 resources..."
          terraform destroy -auto-approve -var-file=tfvars/${{ needs.determine-environment.outputs.tfvars-file }} -target=module.ec2_instance || true
        env:
          GITHUB_TOKEN: ${{ secrets.PRIVATE_REPO_TOKEN }}
          GIT_TOKEN: ${{ secrets.PRIVATE_REPO_TOKEN }}
        continue-on-error: true

      - name: Cleanup summary
        if: always()
        run: |
          echo "## ðŸ§¹ ALB Log Cleanup and Destroy Retry Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ needs.determine-environment.outputs.environment }}" >> $GITHUB_STEP_SUMMARY
          echo "**Reason:** Terraform destroy failed due to non-empty S3 buckets" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Actions Taken:" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ§¹ Cleaned up ALB access log buckets" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ”„ Retried Terraform destroy" >> $GITHUB_STEP_SUMMARY
          if [ "${{ job.status }}" == "failure" ]; then
            echo "- âš ï¸ Attempted force destroy of remaining resources" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### âš ï¸ Manual Cleanup May Be Required" >> $GITHUB_STEP_SUMMARY
            echo "Some resources may still exist in AWS. Please check the AWS Console and clean up manually if needed." >> $GITHUB_STEP_SUMMARY
          else
            echo "- âœ… Successfully destroyed all infrastructure" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "â„¹ï¸ **Note:** Terraform state and DynamoDB tables were preserved" >> $GITHUB_STEP_SUMMARY
  production-approval:
    name: Production Terraform Apply Approval
    runs-on: ubuntu-latest
    needs: [determine-environment, terraform-plan]
    if: |
      needs.determine-environment.outputs.environment == 'production' &&
      needs.terraform-plan.outputs.plan-success == 'true' &&
      github.event_name == 'workflow_dispatch' &&
      github.event.inputs.action == 'plan-and-apply'
    environment: 
      name: production-apply-approval
    steps:
      - name: Production Apply Approval Required
        run: |
          echo "## ðŸ”’ Production Apply Approval" >> $GITHUB_STEP_SUMMARY
          echo "This step requires manual approval before applying changes to production." >> $GITHUB_STEP_SUMMARY
          echo "Please review the terraform plan and approve if the changes are correct." >> $GITHUB_STEP_SUMMARY
          echo "approved=true" >> $GITHUB_OUTPUT
  promote-infrastructure:
    name: Promote Infrastructure
    runs-on: ubuntu-latest
    needs: [determine-environment, deploy]
    if: |
      always() && 
      needs.deploy.outputs.deploy-success == 'true' &&
      (
        (github.event_name == 'workflow_dispatch' && github.event.inputs.create_promotion_pr == 'true') ||
        (github.event_name == 'push' && (github.ref_name == 'dev' || github.ref_name == 'staging'))
      ) &&
      (github.event_name != 'workflow_dispatch' || github.event.inputs.action != 'destroy')
    outputs:
      promotion_created: ${{ steps.promotion.outputs.skip_promotion == 'false' }}
      source_env: ${{ steps.promotion.outputs.source_env }}
      target_env: ${{ steps.promotion.outputs.target_env }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PRIVATE_REPO_TOKEN }}
          fetch-depth: 0

      - name: Setup Git
        run: |
          git config --global user.email "github-actions@github.com"
          git config --global user.name "GitHub Actions"
      - name: Determine promotion target
        id: promotion
        run: |
          SOURCE_ENV="${{ needs.determine-environment.outputs.environment }}"
          
          case $SOURCE_ENV in
            "dev") TARGET_ENV="staging" ;;
            "staging") TARGET_ENV="production" ;;
            "production") TARGET_ENV="" ;;
            *) TARGET_ENV="" ;;
          esac
          
          echo "source_env=$SOURCE_ENV" >> $GITHUB_OUTPUT
          echo "target_env=$TARGET_ENV" >> $GITHUB_OUTPUT
          
          if [ -z "$TARGET_ENV" ]; then
            echo "No promotion target for $SOURCE_ENV environment"
            echo "skip_promotion=true" >> $GITHUB_OUTPUT
          else
            echo "Promotion: $SOURCE_ENV â†’ $TARGET_ENV"
            echo "skip_promotion=false" >> $GITHUB_OUTPUT
          fi
      - name: Create repository labels
        if: steps.promotion.outputs.skip_promotion == 'false'
        run: |
          gh label create "promotion" --description "Infrastructure promotion PR" --color "0052cc" || true
          gh label create "infrastructure" --description "Infrastructure changes" --color "1d76db" || true
          gh label create "automated" --description "Automated PR" --color "0e8a16" || true
          gh label create "dev" --description "Development environment" --color "fbca04" || true
          gh label create "staging" --description "Staging environment" --color "ff9500" || true
          gh label create "production" --description "Production environment" --color "d73a49" || true
        env:
          GH_TOKEN: ${{ secrets.PRIVATE_REPO_TOKEN }}

      - name: Create promotion PR
        if: steps.promotion.outputs.skip_promotion == 'false'
        run: |
          SOURCE_ENV="${{ steps.promotion.outputs.source_env }}"
          TARGET_ENV="${{ steps.promotion.outputs.target_env }}"
          
          PR_BODY="## ðŸš€ Infrastructure Promotion
          **Source Environment:** $SOURCE_ENV
          **Target Environment:** $TARGET_ENV
          **Source Commit:** ${{ github.sha }}
          **Workflow Run:** ${{ github.run_id }}
          **Deployment Status:** âœ… $SOURCE_ENV deployment completed successfully

          ### ðŸ“‹ What's Being Promoted

          This PR promotes infrastructure changes from $SOURCE_ENV to $TARGET_ENV.

          ### ðŸš€ Next Steps

          1. **Review** all infrastructure changes
          2. **Approve and merge** to trigger $TARGET_ENV deployment

          ---
          *This PR was created automatically after successful $SOURCE_ENV deployment.*"
          
          EXISTING_PR=$(gh pr list --base $TARGET_ENV --head $SOURCE_ENV --json number --jq '.[0].number' 2>/dev/null || echo "")
          
          if [ -n "$EXISTING_PR" ]; then
            echo "PR already exists: #$EXISTING_PR"
            echo "Updating existing PR..."
            echo "$PR_BODY" | gh pr edit $EXISTING_PR --title "ðŸš€ Promote infrastructure: $SOURCE_ENV â†’ $TARGET_ENV" --body-file -
          else
            echo "Creating new PR..."
            echo "$PR_BODY" | gh pr create \
              --base $TARGET_ENV \
              --head $SOURCE_ENV \
              --title "ðŸš€ Promote infrastructure: $SOURCE_ENV â†’ $TARGET_ENV" \
              --body-file - \
              --label "promotion,$TARGET_ENV,infrastructure,automated"
          fi
        env:
          GH_TOKEN: ${{ secrets.PRIVATE_REPO_TOKEN }}

      - name: Add promotion summary
        if: steps.promotion.outputs.skip_promotion == 'false'
        run: |
          echo "## ðŸš€ Promotion Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Source Environment:** ${{ steps.promotion.outputs.source_env }}" >> $GITHUB_STEP_SUMMARY
          echo "**Target Environment:** ${{ steps.promotion.outputs.target_env }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… **Status:** Promotion PR created successfully" >> $GITHUB_STEP_SUMMARY

  destroy-status:
    name: Destroy Status Summary
    runs-on: ubuntu-latest
    needs: [determine-environment, terraform-destroy, cleanup-and-retry-destroy]
    if: |
      always() && 
      github.event_name == 'workflow_dispatch' && 
      github.event.inputs.action == 'destroy'
    steps:
      - name: Determine final destroy status
        id: final-status
        run: |
          echo "ðŸ” Checking destroy operation results..."
          echo "Main destroy result: ${{ needs.terraform-destroy.result }}"
          echo "Cleanup retry result: ${{ needs.cleanup-and-retry-destroy.result }}"
          echo "Cleanup retry success: ${{ needs.cleanup-and-retry-destroy.outputs.retry-success }}"
          
          if [ "${{ needs.terraform-destroy.result }}" == "success" ]; then
            echo "âœ… Destroy completed successfully on first attempt"
            echo "status=success" >> $GITHUB_OUTPUT
            echo "message=Infrastructure destroyed successfully" >> $GITHUB_OUTPUT
          elif [ "${{ needs.cleanup-and-retry-destroy.result }}" == "success" ] && [ "${{ needs.cleanup-and-retry-destroy.outputs.retry-success }}" == "true" ]; then
            echo "âœ… Destroy completed successfully after ALB cleanup and retry"
            echo "status=success" >> $GITHUB_OUTPUT
            echo "message=Infrastructure destroyed successfully after ALB log cleanup" >> $GITHUB_OUTPUT
          else
            echo "âŒ Destroy operation failed"
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "message=Infrastructure destroy failed - manual cleanup may be required" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Add final status summary
        run: |
          echo "## ðŸ—‘ï¸ Infrastructure Destroy Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ needs.determine-environment.outputs.environment }}" >> $GITHUB_STEP_SUMMARY
          echo "**Final Status:** ${{ steps.final-status.outputs.status == 'success' && 'âœ… Success' || 'âŒ Failed' }}" >> $GITHUB_STEP_SUMMARY
          echo "**Message:** ${{ steps.final-status.outputs.message }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.terraform-destroy.result }}" == "failure" ] && [ "${{ steps.final-status.outputs.status }}" == "success" ]; then
            echo "### ðŸ”„ Recovery Actions Taken:" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ§¹ ALB access log buckets were cleaned up" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ”„ Terraform destroy was retried successfully" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… All infrastructure has been destroyed" >> $GITHUB_STEP_SUMMARY
          fi
          echo "ðŸ“‹ **Next Step:** Review and merge the PR to deploy to ${{ steps.promotion.outputs.target_env }}" >> $GITHUB_STEP_SUMMARY
      - name: No promotion needed
        if: steps.promotion.outputs.skip_promotion == 'true'
        run: |
          echo "## â„¹ï¸ No Promotion Available" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ steps.promotion.outputs.source_env }}" >> $GITHUB_STEP_SUMMARY
          echo "**Reason:** This is the final environment in the promotion chain" >> $GITHUB_STEP_SUMMARY
  promote-destroy:
    name: Promote Destroy to Next Environment
    runs-on: ubuntu-latest
    needs: [determine-environment, terraform-destroy, cleanup-and-retry-destroy]
    if: |
      always() && 
      (
        (needs.terraform-destroy.result == 'success') ||
        (needs.cleanup-and-retry-destroy.result == 'success' && needs.cleanup-and-retry-destroy.outputs.retry-success == 'true')
      ) &&
      github.event_name == 'workflow_dispatch' && 
      github.event.inputs.action == 'destroy'
    outputs:
      promotion_created: ${{ steps.promotion.outputs.skip_promotion == 'false' }}
      source_env: ${{ steps.promotion.outputs.source_env }}
      target_env: ${{ steps.promotion.outputs.target_env }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PRIVATE_REPO_TOKEN }}
          fetch-depth: 0

      - name: Setup Git
        run: |
          git config --global user.email "github-actions@github.com"
          git config --global user.name "GitHub Actions"

      - name: Determine destroy promotion target
        id: promotion
        run: |
          SOURCE_ENV="${{ needs.determine-environment.outputs.environment }}"
          
          case $SOURCE_ENV in
            "dev") TARGET_ENV="staging" ;;
            "staging") TARGET_ENV="production" ;;
            "production") TARGET_ENV="" ;;
            *) TARGET_ENV="" ;;
          esac
          
          echo "source_env=$SOURCE_ENV" >> $GITHUB_OUTPUT
          echo "target_env=$TARGET_ENV" >> $GITHUB_OUTPUT
          
          if [ -z "$TARGET_ENV" ]; then
            echo "No destroy promotion target for $SOURCE_ENV environment"
            echo "skip_promotion=true" >> $GITHUB_OUTPUT
          else
            echo "Destroy Promotion: $SOURCE_ENV â†’ $TARGET_ENV"
            echo "skip_promotion=false" >> $GITHUB_OUTPUT
          fi

      - name: Create destroy promotion PR
        if: steps.promotion.outputs.skip_promotion == 'false'
        run: |
          SOURCE_ENV="${{ steps.promotion.outputs.source_env }}"
          TARGET_ENV="${{ steps.promotion.outputs.target_env }}"
          
          PR_BODY="## ðŸ—‘ï¸ Infrastructure Destroy Promotion

          **Source Environment:** $SOURCE_ENV (âœ… Successfully Destroyed)
          **Target Environment:** $TARGET_ENV (â³ Pending Destroy)
          **Action:** Destroy infrastructure in $TARGET_ENV environment

          ### ðŸš¨ Destroy Summary for $SOURCE_ENV
          - âœ… Infrastructure successfully destroyed
          - ðŸ§¹ ALB access log buckets cleaned up
          - ðŸ—„ï¸ Terraform state preserved
          - ðŸ”’ Backend resources maintained

          ### âš ï¸ What will happen in $TARGET_ENV:
          - ðŸ—‘ï¸ All EC2 instances will be destroyed
          - ðŸ—‘ï¸ All Load Balancers will be destroyed  
          - ðŸ—‘ï¸ All associated resources will be destroyed
          - ðŸ”’ Terraform state will be preserved

          ### ðŸ“‹ Next Steps:
          1. Review this PR carefully
          2. Merge this PR to trigger destroy in $TARGET_ENV
          3. Approve the destroy operation when prompted
          4. Monitor the destroy process

          **âš ï¸ WARNING: This action cannot be undone!**

          ---
          *This PR was automatically created after successful destroy of $SOURCE_ENV environment.*"

          # Check if PR already exists
          EXISTING_PR=$(gh pr list --base $TARGET_ENV --head $SOURCE_ENV --json number --jq '.[0].number' 2>/dev/null || echo "")
          
          if [ -n "$EXISTING_PR" ]; then
            echo "PR already exists: #$EXISTING_PR"
            echo "Updating existing PR..."
            echo "$PR_BODY" | gh pr edit $EXISTING_PR --title "ðŸ—‘ï¸ Destroy infrastructure: $SOURCE_ENV â†’ $TARGET_ENV" --body-file -
          else
            echo "Creating new destroy promotion PR..."
            echo "$PR_BODY" | gh pr create \
              --title "ðŸ—‘ï¸ Destroy infrastructure: $SOURCE_ENV â†’ $TARGET_ENV" \
              --base $TARGET_ENV \
              --head $SOURCE_ENV \
              --label "destroy,$TARGET_ENV,infrastructure,automated"
          fi
        env:
          GH_TOKEN: ${{ secrets.PRIVATE_REPO_TOKEN }}

      - name: Add destroy promotion summary
        if: steps.promotion.outputs.skip_promotion == 'false'
        run: |
          echo "## ðŸ—‘ï¸ Destroy Promotion Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Source Environment:** ${{ steps.promotion.outputs.source_env }} (âœ… Destroyed)" >> $GITHUB_STEP_SUMMARY
          echo "**Target Environment:** ${{ steps.promotion.outputs.target_env }} (â³ Pending)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… **Status:** Destroy promotion PR created successfully" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“‹ **Next Step:** Review and merge the PR to destroy ${{ steps.promotion.outputs.target_env }}" >> $GITHUB_STEP_SUMMARY

      - name: No destroy promotion needed
        if: steps.promotion.outputs.skip_promotion == 'true'
        run: |
          echo "## â„¹ï¸ No Destroy Promotion Available" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ steps.promotion.outputs.source_env }}" >> $GITHUB_STEP_SUMMARY
          echo "**Reason:** This is the final environment in the destroy chain" >> $GITHUB_STEP_SUMMARY

  workflow-summary:
    name: Workflow Summary
    runs-on: ubuntu-latest
    needs: [determine-environment, deploy, promote-infrastructure, terraform-destroy, cleanup-and-retry-destroy, promote-destroy]
    if: |
      always() && 
      (
        (needs.deploy.result != 'skipped') ||
        (needs.terraform-destroy.result != 'skipped') ||
        (needs.cleanup-and-retry-destroy.result != 'skipped')
      )
    steps:
      - name: Generate workflow summary
        run: |
          echo "## ðŸš€ Terraform Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ needs.determine-environment.outputs.environment }}" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.deploy.result }}" = "success" ]; then
            echo "âœ… **Deploy Status:** Successful" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Deploy Status:** Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.promote-infrastructure.result }}" = "success" ]; then
            echo "âœ… **Promotion Status:** Successful" >> $GITHUB_STEP_SUMMARY
            if [ "${{ needs.promote-infrastructure.outputs.promotion_created }}" = "true" ]; then
              echo "ðŸ“‹ **Next Step:** Review and merge PR: ${{ needs.promote-infrastructure.outputs.source_env }} â†’ ${{ needs.promote-infrastructure.outputs.target_env }}" >> $GITHUB_STEP_SUMMARY
            fi
          elif [ "${{ needs.promote-infrastructure.result }}" = "skipped" ]; then
            echo "â­ï¸ **Promotion Status:** Skipped" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Promotion Status:** Failed" >> $GITHUB_STEP_SUMMARY
          fi